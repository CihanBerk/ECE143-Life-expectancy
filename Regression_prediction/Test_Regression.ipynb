{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pandas.read_csv('df_NaN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ericl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "D:\\Users\\ericl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#set Developing to 1 and Developed to 0\n",
    "data['Status'][data['Status'] == 'Developing']  = 0\n",
    "data['Status'][data['Status'] == 'Developed']  = 1\n",
    "\n",
    "\n",
    "#data normalization for further processing\n",
    "#data.apply(normalization,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract target data\n",
    "\n",
    "target=data['Life expectancy ']\n",
    "target=np.array(target)\n",
    "#Ignore the country and year effect\n",
    "data=data.drop(['Country','Year','Unnamed: 0','Life expectancy '],axis=1)\n",
    "#normalize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract normalization parameters and normalize original data\n",
    "dataMax=np.array(data.max())\n",
    "dataMin=np.array(data.min())\n",
    "dataMean=np.array(data.mean())\n",
    "data=data.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "# is it necessary to normalize label data?\n",
    "life=target.copy()\n",
    "#target=(target-target.min())/(target.max()-target.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(train_x, train_y):\n",
    "    \"\"\"\n",
    "    use least square method to reach the regression model\n",
    "    inputï¼š\n",
    "          train_x, training data\n",
    "          train_y, labels in training data\n",
    "    \"\"\"  \n",
    "    weights = inv(np.dot(train_x.T ,train_x).astype(float)).dot(train_x.T).dot(train_y) \n",
    "    return weights.astype(float)\n",
    "\n",
    "least_square_weights=least_square(train_x,target)\n",
    "#prove to be very inaccurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef gradient_descent(train_x, train_y, maxCycle, alpha):\\n     \\n    numSamples, numFeatures = np.shape(train_x)\\n    weights = np.zeros((numFeatures,1))\\n     \\n    for i in range(maxCycle):\\n        h = train_x.dot(weights)\\n        err = h - train_y           \\n        weights = weights - (alpha*err.T.dot(train_x)).T\\n        print(err)\\n    return weights.astype(float)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def gradient_descent(train_x, train_y, maxCycle, alpha):\n",
    "     \n",
    "    numSamples, numFeatures = np.shape(train_x)\n",
    "    weights = np.zeros((numFeatures,1))\n",
    "     \n",
    "    for i in range(maxCycle):\n",
    "        h = train_x.dot(weights)\n",
    "        err = h - train_y           \n",
    "        weights = weights - (alpha*err.T.dot(train_x)).T\n",
    "        print(err)\n",
    "    return weights.astype(float)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a shallow neural network with pytorch\n",
    "\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch .nn. functional as F\n",
    "\n",
    "torch_x=Variable(torch.from_numpy(train_x.astype(float)),requires_grad = True)\n",
    "torch_y=Variable(torch.from_numpy(target),requires_grad = False)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(19,57).double()\n",
    "        #self.fc2 = torch.nn.Linear(19,19).double()\n",
    "        self.fc2 = torch.nn.Linear(57,1).double()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        #x=F.relu(self.fc2(x))\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model=Model()\n",
    "criterion=torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "            \n",
    "         \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model(torch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0013],\n",
       "        [-0.0079],\n",
       "        [-0.0104],\n",
       "        ...,\n",
       "        [ 0.0648],\n",
       "        [ 0.0123],\n",
       "        [-0.0420]], dtype=torch.float64, grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_y=torch_y.reshape([2938,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2938, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ericl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(89180.9994, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "1 tensor(37045.3149, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "2 tensor(9327.3437, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "3 tensor(4021.1235, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "4 tensor(2594.2878, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "5 tensor(2653.7137, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "6 tensor(2245.8566, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "7 tensor(1802.4337, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "8 tensor(1877.6328, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "9 tensor(657.9297, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "10 tensor(648.8578, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "11 tensor(1875.6460, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "12 tensor(1284.7494, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "13 tensor(1064.6004, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "14 tensor(969.6589, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "15 tensor(843.1788, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "16 tensor(1206.4025, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "17 tensor(615.1422, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "18 tensor(682.3577, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "19 tensor(686.0904, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "20 tensor(962.8351, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "21 tensor(814.2032, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "22 tensor(818.9054, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "23 tensor(1137.7201, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "24 tensor(778.6835, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "25 tensor(557.6962, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "26 tensor(572.5485, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "27 tensor(630.6603, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "28 tensor(678.2322, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "29 tensor(707.4574, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "30 tensor(549.9989, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "31 tensor(781.6873, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "32 tensor(499.5926, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "33 tensor(760.8926, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "34 tensor(569.4861, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "35 tensor(528.6697, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "36 tensor(632.4076, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "37 tensor(516.3605, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "38 tensor(505.5620, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "39 tensor(475.0446, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "40 tensor(586.5099, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "41 tensor(714.0126, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "42 tensor(978.2285, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "43 tensor(483.3649, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "44 tensor(583.5899, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "45 tensor(593.6680, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "46 tensor(460.1163, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "47 tensor(511.4273, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "48 tensor(490.2161, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "49 tensor(602.0361, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "50 tensor(616.3260, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "51 tensor(658.9124, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "52 tensor(678.1519, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "53 tensor(528.9910, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "54 tensor(516.1155, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "55 tensor(630.0091, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "56 tensor(428.6913, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "57 tensor(631.6971, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "58 tensor(408.8568, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "59 tensor(629.1354, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "60 tensor(531.6164, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "61 tensor(704.7437, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "62 tensor(591.3601, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "63 tensor(445.2806, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "64 tensor(426.3998, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "65 tensor(407.2946, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "66 tensor(459.5466, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "67 tensor(397.2399, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "68 tensor(489.1896, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "69 tensor(462.3242, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "70 tensor(460.0639, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "71 tensor(457.6269, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "72 tensor(432.3458, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "73 tensor(423.7434, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "74 tensor(415.6240, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "75 tensor(557.2078, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "76 tensor(510.2242, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "77 tensor(540.9943, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "78 tensor(496.0632, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "79 tensor(497.0846, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "80 tensor(448.0228, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "81 tensor(409.9381, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "82 tensor(414.4379, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "83 tensor(476.3438, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "84 tensor(386.5873, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "85 tensor(474.0091, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "86 tensor(393.8729, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "87 tensor(423.8281, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "88 tensor(487.6721, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "89 tensor(403.4067, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "90 tensor(461.7835, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "91 tensor(386.9421, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "92 tensor(467.6074, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "93 tensor(405.8027, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "94 tensor(429.4609, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "95 tensor(438.7552, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "96 tensor(496.1081, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "97 tensor(423.7789, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "98 tensor(350.7880, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "99 tensor(503.4125, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "100 tensor(455.3415, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "101 tensor(455.2858, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "102 tensor(452.7252, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "103 tensor(398.0850, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "104 tensor(392.9913, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "105 tensor(384.3230, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "106 tensor(456.3073, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "107 tensor(342.6374, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "108 tensor(458.3512, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "109 tensor(398.4253, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "110 tensor(382.3986, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "111 tensor(358.7730, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "112 tensor(412.4374, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "113 tensor(351.8030, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "114 tensor(366.3874, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "115 tensor(441.1057, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "116 tensor(333.3270, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "117 tensor(423.4907, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "118 tensor(427.0579, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "119 tensor(506.4552, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "120 tensor(426.1252, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "121 tensor(399.3957, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "122 tensor(350.2860, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "123 tensor(322.9352, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "124 tensor(454.9026, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "125 tensor(347.6848, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "126 tensor(360.5448, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "127 tensor(397.2035, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "128 tensor(345.2074, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "129 tensor(381.5856, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "130 tensor(407.3052, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "131 tensor(364.6202, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "132 tensor(332.1757, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "133 tensor(346.6535, dtype=torch.float64, grad_fn=<ThAddBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 tensor(363.8457, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "135 tensor(301.5899, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "136 tensor(350.5402, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "137 tensor(366.8414, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "138 tensor(388.9246, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "139 tensor(338.3568, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "140 tensor(325.8183, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "141 tensor(357.3830, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "142 tensor(308.2303, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "143 tensor(381.6113, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "144 tensor(384.7118, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "145 tensor(350.2923, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "146 tensor(314.0300, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "147 tensor(302.5803, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "148 tensor(351.1581, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "149 tensor(349.1174, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "150 tensor(322.7311, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "151 tensor(337.6987, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "152 tensor(307.0583, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "153 tensor(324.9653, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "154 tensor(310.1548, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "155 tensor(317.6683, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "156 tensor(299.9086, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "157 tensor(320.7497, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "158 tensor(300.0707, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "159 tensor(319.6107, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "160 tensor(313.9834, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "161 tensor(305.8840, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "162 tensor(310.8742, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "163 tensor(288.8504, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "164 tensor(268.2679, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "165 tensor(308.1585, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "166 tensor(279.5674, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "167 tensor(310.4677, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "168 tensor(300.5561, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "169 tensor(338.9063, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "170 tensor(311.2177, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "171 tensor(280.4466, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "172 tensor(289.2429, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "173 tensor(302.0471, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "174 tensor(280.7495, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "175 tensor(302.5824, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "176 tensor(280.0356, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "177 tensor(315.9467, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "178 tensor(289.3844, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "179 tensor(271.0023, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "180 tensor(284.2914, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "181 tensor(264.1722, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "182 tensor(275.6469, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "183 tensor(302.1960, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "184 tensor(257.0631, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "185 tensor(271.7762, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "186 tensor(316.0281, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "187 tensor(297.3905, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "188 tensor(251.8119, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "189 tensor(319.9691, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "190 tensor(305.4251, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "191 tensor(299.0542, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "192 tensor(275.4610, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "193 tensor(268.5266, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "194 tensor(301.0083, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "195 tensor(262.4992, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "196 tensor(263.5560, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "197 tensor(269.1741, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "198 tensor(275.8313, dtype=torch.float64, grad_fn=<ThAddBackward>)\n",
      "199 tensor(267.6120, dtype=torch.float64, grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "T=200\n",
    "B=100\n",
    "NB=30\n",
    "N=2938\n",
    "for epoch in range(T):\n",
    "    running_loss=0.0\n",
    "    idxminibatches = np. random . permutation (NB)\n",
    "    for k in range(NB):\n",
    "        i = idxminibatches [k]\n",
    "        idxsmp=np.arange(B*i,min(B*(i+1),N))\n",
    "        inputs = torch_x[idxsmp]\n",
    "        labels = torch_y[idxsmp]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred=model(inputs)\n",
    "\n",
    "        loss=criterion(y_pred,labels)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss+=loss[0]\n",
    "        if k==29:\n",
    "            \n",
    "            print(epoch,running_loss)\n",
    "            running_loss=0.0\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model(torch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62.9524],\n",
       "        [63.6344],\n",
       "        [63.9052],\n",
       "        ...,\n",
       "        [44.7711],\n",
       "        [45.5484],\n",
       "        [46.6790]], dtype=torch.float64, grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
